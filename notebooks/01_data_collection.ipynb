{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e9995c12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/envs/env39/lib/python3.9/site-packages/pandas/core/arrays/masked.py:60: UserWarning: Pandas requires version '1.3.6' or newer of 'bottleneck' (version '1.3.5' currently installed).\n",
      "  from pandas.core import (\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pickle\n",
    "import json\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3026eded",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Headers for HTTP requests\n",
    "HEADERS = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/111.0.0.0 Safari/537.36',\n",
    "    'Accept-Language': 'en-US,en;q=0.9', \n",
    "    'Accept-Encoding': 'gzip, deflate, br',\n",
    "    'Connection': 'keep-alive'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c5ba7367",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Load catalog page data\n",
    "def load_catalog_data():\n",
    "    \"\"\"Load Sephora catalog page data from JSON files\"\"\"\n",
    "    catalog_path = \"config/sephora_pages/\"\n",
    "    catalog_data = []\n",
    "    \n",
    "    for file_name in os.listdir(catalog_path):\n",
    "        if file_name.endswith('.json'):\n",
    "            with open(os.path.join(catalog_path, file_name)) as f:\n",
    "                catalog_data.append(json.load(f))\n",
    "                \n",
    "    return catalog_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "17c28821",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_catalog_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff5ad52a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create DataFrame from catalog data\n",
    "def create_product_df(catalog_data):\n",
    "    \"\"\"Create DataFrame containing product information from catalog pages\"\"\"\n",
    "    return pd.concat([pd.json_normalize(page['products']) for page in catalog_data])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b85da7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Get product details\n",
    "def get_product_details(product_url, headers=HEADERS):\n",
    "    \"\"\"\n",
    "    Get detailed product information from product page\n",
    "    \n",
    "    Parameters:\n",
    "        product_url (str): URL of the product page\n",
    "        headers (dict): HTTP request headers\n",
    "        \n",
    "    Returns:\n",
    "        dict: Dictionary containing product details, or None if extraction fails\n",
    "    \"\"\"\n",
    "    product_info = {\n",
    "        'product_id': None,\n",
    "        'size_and_item': None,\n",
    "        'category': None, \n",
    "        'price': None,\n",
    "        'love_count': None,\n",
    "        'reviews_count': None,\n",
    "        'swatch_images': [],\n",
    "        'sku_color_mapping': {},\n",
    "        'page_content': None\n",
    "    }\n",
    "    \n",
    "    try:\n",
    "        # Make HTTP request\n",
    "        response = requests.get(product_url, headers=headers, timeout=15)\n",
    "        soup = BeautifulSoup(response.text, 'html.parser')\n",
    "        \n",
    "        # Extract Product ID from URL\n",
    "        try:\n",
    "            product_info['product_id'] = re.findall(r'P[0-9]{3,6}', product_url)[0]\n",
    "        except (IndexError, AttributeError):\n",
    "            print(f\"Could not extract product ID from {product_url}\")\n",
    "\n",
    "        # Extract Category Path\n",
    "        try:\n",
    "            category_element = soup.find_all(attrs={'data-comp': 'ProductBreadCrumbs BreadCrumbs '})[0]\n",
    "            category_links = category_element.find_all('a')\n",
    "            category_names = [link.string for link in category_links if link.string]\n",
    "            product_info['category'] = ' > '.join(category_names)\n",
    "        except (IndexError, AttributeError):\n",
    "            print(f\"Could not extract category for product {product_info['product_id']}\")\n",
    "\n",
    "        # Extract Size and Item Details\n",
    "        try:\n",
    "            size_element = soup.find(attrs={\"data-at\": \"sku_size_label\"})\n",
    "            product_info['size_and_item'] = size_element.get_text() if size_element else None\n",
    "        except AttributeError:\n",
    "            print(f\"Could not extract size info for product {product_info['product_id']}\")\n",
    "\n",
    "        # Extract Price\n",
    "        try:\n",
    "            price_element = soup.find_all(attrs={'data-comp': 'Price '})[0]\n",
    "            product_info['price'] = price_element.get_text() if price_element else None\n",
    "        except (IndexError, AttributeError):\n",
    "            print(f\"Could not extract price for product {product_info['product_id']}\")\n",
    "\n",
    "        # Extract Love Count\n",
    "        try:\n",
    "            love_element = soup.find('span', attrs={\"class\": \"css-jk94q9\"})\n",
    "            product_info['love_count'] = love_element.get_text() if love_element else None\n",
    "        except AttributeError:\n",
    "            print(f\"Could not extract love count for product {product_info['product_id']}\")\n",
    "\n",
    "        # Extract Review Count\n",
    "        try:\n",
    "            reviews_element = soup.find('span', attrs={'data-at': 'number_of_reviews'})\n",
    "            product_info['reviews_count'] = reviews_element.get_text() if reviews_element else None\n",
    "        except AttributeError:\n",
    "            print(f\"Could not extract review count for product {product_info['product_id']}\")\n",
    "\n",
    "        # Extract Swatch Images and Color Mapping\n",
    "        try:\n",
    "            swatch_groups = soup.find_all(attrs={'data-comp': 'SwatchGroup '})\n",
    "            \n",
    "            for group in swatch_groups:\n",
    "                for button in group.find_all('button'):\n",
    "                    # Get color name from aria-label\n",
    "                    color_name = button.get('aria-label')\n",
    "                    \n",
    "                    # Get image and SKU info\n",
    "                    img = button.find('img')\n",
    "                    if img and 'src' in img.attrs:\n",
    "                        product_info['swatch_images'].append(img['src'])\n",
    "                        \n",
    "                        # Extract SKU from image URL\n",
    "                        sku_match = re.findall(r's[0-9]+', img['src'])\n",
    "                        if sku_match and color_name:\n",
    "                            sku_id = sku_match[0][1:]  # Remove 's' prefix\n",
    "                            product_info['sku_color_mapping'][sku_id] = color_name\n",
    "                            \n",
    "        except Exception as e:\n",
    "            print(f\"Could not extract swatch info for product {product_info['product_id']}: {str(e)}\")\n",
    "\n",
    "        # Store full page content for potential future use\n",
    "        product_info['page_content'] = soup\n",
    "        \n",
    "        return product_info\n",
    "\n",
    "    except requests.exceptions.RequestException as e:\n",
    "        print(f\"Network error while fetching {product_url}: {str(e)}\")\n",
    "        return None\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Unexpected error processing {product_url}: {str(e)}\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333d8968",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Download images \n",
    "def download_product_image(image_url, output_dir, headers=HEADERS):\n",
    "    \"\"\"Download product image from URL\"\"\"\n",
    "    try:\n",
    "        # Create output directory if it doesn't exist\n",
    "        os.makedirs(output_dir, exist_ok=True)\n",
    "        \n",
    "        # Extract filename from URL\n",
    "        image_name = image_url.split('/')[-1]\n",
    "        image_path = os.path.join(output_dir, image_name)\n",
    "        \n",
    "        # Skip if image already exists\n",
    "        if os.path.exists(image_path):\n",
    "            return\n",
    "            \n",
    "        # Download image\n",
    "        response = requests.get(image_url, headers=headers)\n",
    "        if response.status_code == 200:\n",
    "            with open(image_path, 'wb') as f:\n",
    "                f.write(response.content)\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading image {image_url}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3ea09e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_product_reviews(product_id, start_date=None, limit=100, max_retries=3):\n",
    "    \"\"\"\n",
    "    Get product reviews using Bazaarvoice API\n",
    "    \n",
    "    Parameters:\n",
    "        product_id (str): Product ID to get reviews for\n",
    "        start_date (str): ISO format date string to filter reviews after this date\n",
    "        limit (int): Number of reviews per request\n",
    "        max_retries (int): Maximum number of retry attempts for failed requests\n",
    "        \n",
    "    Returns:\n",
    "        tuple: (product_info, reviews_list)\n",
    "    \"\"\"\n",
    "    \n",
    "    # API Configuration\n",
    "    API_CONFIG = {\n",
    "        \"host\": \"api.bazaarvoice.com\",\n",
    "        \"token\": \"caHFIcND7how0aLS6wzoJhq0PcvkFllbfQUmnsxU3BMZo\",\n",
    "        \"version\": \"5.4\"\n",
    "    }\n",
    "    \n",
    "    # Build API URL and base parameters\n",
    "    url = f\"https://{API_CONFIG['host']}/data/reviews.json\"\n",
    "    params = {\n",
    "        'Filter': [f'ProductId:{product_id}'],\n",
    "        'Sort': 'SubmissionTime:desc',\n",
    "        'Limit': limit,\n",
    "        'Offset': 0,\n",
    "        'Include': 'Products,Comments',\n",
    "        'Stats': 'Reviews',\n",
    "        'passkey': API_CONFIG['token'],\n",
    "        'apiversion': API_CONFIG['version'],\n",
    "        'Locale': 'en_US'\n",
    "    }\n",
    "    \n",
    "    # Add date filter if specified\n",
    "    if start_date:\n",
    "        timestamp = int(datetime.strptime(start_date, '%Y-%m-%dT%H:%M:%S.%f%z').timestamp())\n",
    "        params['Filter'].append(f'SubmissionTime:gt:{timestamp}')\n",
    "    \n",
    "    reviews = []\n",
    "    product_info = []\n",
    "    retry_count = 0\n",
    "    \n",
    "    try:\n",
    "        while True:\n",
    "            # Update offset for pagination\n",
    "            params['Offset'] = len(reviews)\n",
    "            \n",
    "            # Make API request with retry logic\n",
    "            for attempt in range(max_retries):\n",
    "                try:\n",
    "                    response = requests.get(url, params=params, timeout=15)\n",
    "                    response.raise_for_status()  # Raise exception for bad status codes\n",
    "                    break\n",
    "                except requests.exceptions.RequestException as e:\n",
    "                    if attempt == max_retries - 1:  # Last attempt\n",
    "                        raise\n",
    "                    time.sleep(2 ** attempt)  # Exponential backoff\n",
    "                    continue\n",
    "            \n",
    "            # Parse response\n",
    "            try:\n",
    "                data = response.json()\n",
    "            except json.JSONDecodeError as e:\n",
    "                logger.error(f\"Failed to decode JSON for product {product_id}: {str(e)}\")\n",
    "                break\n",
    "                \n",
    "            # Check for API errors\n",
    "            if data.get('HasErrors', False):\n",
    "                error_msg = data.get('Errors', ['Unknown error'])[0]\n",
    "                logger.error(f\"API error for product {product_id}: {error_msg}\")\n",
    "                break\n",
    "                \n",
    "            # Extract product information (only on first page)\n",
    "            if len(reviews) == 0:\n",
    "                try:\n",
    "                    product_info = data['Includes']['Products'].get(product_id, {})\n",
    "                except KeyError:\n",
    "                    product_info = {}\n",
    "            \n",
    "            # Extract reviews\n",
    "            new_reviews = data.get('Results', [])\n",
    "            if not new_reviews:\n",
    "                break\n",
    "                \n",
    "            # Process each review\n",
    "            for review in new_reviews:\n",
    "                processed_review = {\n",
    "                    'review_id': review.get('Id'),\n",
    "                    'product_id': product_id,\n",
    "                    'rating': review.get('Rating'),\n",
    "                    'title': review.get('Title'),\n",
    "                    'review_text': review.get('ReviewText'),\n",
    "                    'submission_time': review.get('SubmissionTime'),\n",
    "                    'last_modified_time': review.get('LastModificationTime'),\n",
    "                    'author': {\n",
    "                        'name': review.get('UserNickname'),\n",
    "                        'location': review.get('UserLocation'),\n",
    "                    },\n",
    "                    'is_verified_purchaser': review.get('IsVerifiedPurchaser', False),\n",
    "                    'total_feedback_count': review.get('TotalFeedbackCount', 0),\n",
    "                    'total_positive_feedback_count': review.get('TotalPositiveFeedbackCount', 0),\n",
    "                    'total_negative_feedback_count': review.get('TotalNegativeFeedbackCount', 0),\n",
    "                    'context_data_values': {\n",
    "                        item['Id']: item.get('Value')\n",
    "                        for item in review.get('ContextDataValues', [])\n",
    "                    }\n",
    "                }\n",
    "                \n",
    "                # Add photos if available\n",
    "                if 'Photos' in review:\n",
    "                    processed_review['photos'] = [\n",
    "                        {\n",
    "                            'id': photo.get('Id'),\n",
    "                            'url': photo.get('Sizes', {}).get('normal', {}).get('Url'),\n",
    "                            'caption': photo.get('Caption')\n",
    "                        }\n",
    "                        for photo in review.get('Photos', [])\n",
    "                    ]\n",
    "                \n",
    "                reviews.append(processed_review)\n",
    "            \n",
    "            # Check if we have all reviews\n",
    "            total_results = data.get('TotalResults', 0)\n",
    "            if len(reviews) >= total_results:\n",
    "                break\n",
    "                \n",
    "            # Rate limiting\n",
    "            time.sleep(0.2)\n",
    "            \n",
    "        logger.info(f\"Retrieved {len(reviews)} reviews for product {product_id}\")\n",
    "        return product_info, reviews\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error retrieving reviews for product {product_id}: {str(e)}\")\n",
    "        return product_info, reviews\n",
    "\n",
    "# Helper function to save reviews\n",
    "def save_reviews(product_id, product_info, reviews, output_dir='data/raw/sephora_reviews/'):\n",
    "    \"\"\"Save product reviews to JSON file\"\"\"\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    output_data = {\n",
    "        'product_info': product_info,\n",
    "        'reviews': reviews\n",
    "    }\n",
    "    \n",
    "    filename = f\"{product_id}_reviews.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(output_data, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3c2c1e5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save product details to file\n",
    "def save_product_details(product_details, output_dir='data/raw/sephora_product_pages/'):\n",
    "    \"\"\"Save product details to JSON file\"\"\"\n",
    "    if not product_details or not product_details['product_id']:\n",
    "        return\n",
    "        \n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "    \n",
    "    # Convert BeautifulSoup object to string to make it JSON serializable\n",
    "    product_details['page_content'] = str(product_details['page_content'])\n",
    "    \n",
    "    filename = f\"{product_details['product_id']}.json\"\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        json.dump(product_details, f, ensure_ascii=False, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e54de3b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Load catalog data and create product DataFrame\n",
    "catalog_data = load_catalog_data()\n",
    "product_df = create_product_df(catalog_data)\n",
    "\n",
    "# 2. Get product details\n",
    "for product_id in tqdm(product_df['productId'].unique()):\n",
    "    product_url = f'https://www.sephora.com/product/{product_id}'\n",
    "    product_details = get_product_details(product_url)\n",
    "    if product_details:\n",
    "        save_product_details(product_details)\n",
    "\n",
    "# 3. Download images\n",
    "image_urls = get_all_image_urls(product_df)\n",
    "for url in tqdm(image_urls):\n",
    "    download_product_image(url, 'data/raw/sephora_images/') \n",
    "\n",
    "# 4. Get reviews\n",
    "for product_id in tqdm(product_df['productId']):\n",
    "    product_info, reviews = get_product_reviews(product_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edcfb225",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec73c5b5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71051a94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93d7e2ff",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34027b7c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6916a886",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e17cbff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env39",
   "language": "python",
   "name": "env39"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
